{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data cleaning function\n",
    "\n",
    "def clean(filename):\n",
    "    import re\n",
    "    clean = []\n",
    "    with open(filename) as inp:\n",
    "        for line in inp:\n",
    "            line = line.strip('\\n')\n",
    "            text = line\n",
    "            words = re.split(r'[!\"\\#\\$%\\&\\'\\(\\)\\*\\+\\-\\.[\\/\\]\\:\\;\\<\\=\\>\\?\\@\\[\\\\\\]\\^\\_\\`\\{\\|\\}\\~ ]', text)\n",
    "            pattern = '[0-9]'\n",
    "            words = [re.sub(pattern, '', i) for i in words] \n",
    "            while('' in words): \n",
    "                words.remove(\"\") \n",
    "            words = [word.lower() for word in words]\n",
    "            strippednew = [re.sub(r'[!\"\\#\\$%\\&\\'\\(\\)\\*\\+\\,\\-\\.[\\/\\]\\:\\;\\<\\=\\>\\?\\@\\[\\\\\\]\\^\\_\\`\\{\\|\\}\\~]', '',w) for w in words]\n",
    "            clean.append(strippednew)\n",
    "        return clean\n",
    "    inp.close()\n",
    "    op.close()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data split\n",
    "# function\n",
    "\n",
    "def split(dataset):\n",
    "    total = len(dataset)\n",
    "    part1 = []\n",
    "    part2 = []\n",
    "    part1 = dataset[0:4998]\n",
    "    part2 = dataset[4999:total]\n",
    "    return part1, part2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mapper\n",
    "# function 1\n",
    "\n",
    "def mapper1(part1):\n",
    "    output1 = [ele for ele in part1 if ele != []]\n",
    "    output11 = []\n",
    "    for word in output1:\n",
    "        for item in word:\n",
    "            output11.append( (item, 1) )\n",
    "    return output11 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mapper\n",
    "# function 2\n",
    "\n",
    "def mapper2(part2):\n",
    "    output2 = [ele for ele in part2 if ele != []]\n",
    "    output12 = []\n",
    "    for word in output2:\n",
    "        for item in word:\n",
    "            output12.append( (item, 1) )\n",
    "    return output12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sort\n",
    "# function\n",
    "\n",
    "\n",
    "def sort(output11, output12):\n",
    "    outputfull = output11 + output12\n",
    "    outputfull.sort(key = lambda x: x[0]) \n",
    "    outputfull = list(filter(lambda t: '' not in t, outputfull))\n",
    "    return outputfull"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Partition\n",
    "# function\n",
    "\n",
    "def partition(outputfull):\n",
    "    atom = []\n",
    "    ntoz = []\n",
    "    i = 0\n",
    "    y = ['a','b','c','d','e','f','g','h','i','j','k','l','m']\n",
    "    z = ['n','o','p','q','r','s','t','u','v','w','x','y','z']\n",
    "    for x in outputfull:\n",
    "        if x[i][0].startswith(tuple(y)):\n",
    "            atom.append(x)\n",
    "        elif x[i][0].startswith(tuple(z)):\n",
    "            ntoz.append(x)\n",
    "    i += 1\n",
    "    return atom, ntoz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reducer\n",
    "# function 1\n",
    "\n",
    "\n",
    "def reducer1(atom):\n",
    "    atomuniq = list(set(atom))\n",
    "    atomuniq.sort(key = lambda x: x[0]) \n",
    "    uniqAtoM = [x[0] for x in atomuniq]\n",
    "    allAtoM = [x[0] for x in atom]\n",
    "    a2m = []\n",
    "    sum = 0\n",
    "    for elem in uniqAtoM:\n",
    "        sum = 0\n",
    "        for item in allAtoM:\n",
    "            if elem == item:\n",
    "                sum +=1\n",
    "        a2m.append((elem, sum))\n",
    "    return a2m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reducer\n",
    "# function 2\n",
    "\n",
    "\n",
    "def reducer2(ntoz):\n",
    "    ntozuniq = list(set(ntoz))\n",
    "    ntozuniq.sort(key = lambda x: x[0]) \n",
    "    uniqNtoZ = [x[0] for x in ntozuniq]\n",
    "    allNtoZ = [x[0] for x in ntoz]\n",
    "    n2z = []\n",
    "    sum = 0\n",
    "    for elem in uniqNtoZ:\n",
    "        sum = 0\n",
    "        for item in allNtoZ:\n",
    "            if elem == item:\n",
    "                sum +=1\n",
    "        n2z.append((elem, sum))\n",
    "    return n2z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main\n",
    "# function\n",
    "\n",
    "\n",
    "def main():\n",
    "    cleaned_data = clean(r'C:\\Users\\joshm\\Desktop\\pride.txt')\n",
    "    split1, split2 = split(cleaned_data)\n",
    "    map1 = mapper1(split1)\n",
    "    map2 = mapper2(split2)\n",
    "    sortout = sort(map1, map2)\n",
    "    atom, ntoz = partition(sortout)\n",
    "    a2m = reducer1(atom)\n",
    "    n2z = reducer2(ntoz)\n",
    "    reducFinal = a2m + n2z\n",
    "    import pandas as pd\n",
    "    df = pd.DataFrame(reducFinal, columns=[\"Word\", \"Frequency\"])\n",
    "    df.to_csv('FinalWordCount.csv', index=False)\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
